fos_elastica:
    serializer: ~
    clients:
        default:
            host: "%elasticsearch.host%"
            port: "%elasticsearch.port%"
            retryOnConflict: 2
            logger: false
    indexes:
        search:
            index_name: "%elasticsearch.index_name%_%kernel.environment%"
            settings:
                index:
                    analysis:
                        tokenizer:
                            n_gram:
                                type: nGram
                                min_gram: 3
                                max_gram: 20
                                token_chars: [letter, digit]
                        filter:
                            # Common
                            delimiter:
                                type: word_delimiter
                            # EN
                            en_stop:
                                type: stop
                                stopwords: [_english_]
                                ignore_case: true
                            en_stemmer:
                                type: stemmer
                                language: minimal_english
                            # FR
                            fr_stop:
                                type: stop
                                stopwords: [_french_]
                                ignore_case: true
                            fr_stemmer:
                                type: stemmer
                                language: minimal_french
                            fr_elision:
                                type: elision
                                articles: [l, m, t, qu, n, s, j, d, c, jusqu, quoiqu, lorsqu, puisqu]
                        analyzer:
                            # Default
                            index:
                                type: custom
                                tokenizer: n_gram
                                filter: [asciifolding, lowercase, delimiter]
                            search:
                                type: custom
                                tokenizer: standard
                                filter: [asciifolding, lowercase, delimiter]
                            # EN
                            en_index:
                                type: custom
                                char_filter: [html_strip]
                                tokenizer: n_gram
                                filter: [en_stop, asciifolding, lowercase, delimiter, en_stemmer]
                            en_search:
                                type: custom
                                tokenizer: standard
                                filter: [en_stop, asciifolding, lowercase, delimiter, en_stemmer]
                            # FR
                            fr_index:
                                type: custom
                                char_filter: [html_strip]
                                tokenizer: n_gram
                                filter: [fr_stop, asciifolding, lowercase, fr_elision, delimiter, fr_stemmer]
                            fr_search:
                                type: custom
                                tokenizer: standard
                                filter: [fr_stop, asciifolding, lowercase, fr_elision, delimiter, fr_stemmer]
